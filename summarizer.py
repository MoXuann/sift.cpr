# -*- coding: utf-8 -*-
"""INTUITION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFE8OIqR8hR0YVEplzqp8N7r1W4VxzAN
"""

# review_str = """
# A very nice laptop and it feels premium. The material is mostly plastic but I appreciate that for the reduced weight. This is my first Ryzen machine, and I am mainly using it to run a Citrix environment (cloud-based VM). Today will be the first full 8+-hour day using it. The power brick is not excessively huge. About the width and height of a standard post-it note, and 1" thick. The cable is over 5' (my measuring tape only goes to 60") closer to 6'. It has a barrel connector so unfortunately, it is not easily replaceable with a USB C cable (next model, pretty please?)
#
# The keyboard is backlit, which is nice for the darker surrounding often found in my home office. Typing is okay, but there is a noticeably shorter key travel than other laptops/Chromebooks I have used. I'm getting used to it, but like with any new keyboard, there will be a small learning curve. I keep missing the home row and using the Caps Lock as the A key for some reason. I'm using it on a small folding table at the moment so that could be the reason.
#
# The screen is nice, it is a 16:9 full HD screen, and for me, I prefer 16:9 to the new standard of 2:3 (more of a square). It is not bright though. Not at all. Even with the curtains open, there are times when I am still trying to get more brightness out of this display. It's not unusable, but forget trying to use it outdoors.
#
# Touch is fine and works. I did not buy this to use as a tablet though but Windows 10 has some processing going on in the background that makes it easy to hit small buttons in applications without being terribly accurate. The pen is interesting to me as it is roughly the size of a real pen [cough Samsung*cough*] which makes it easier to draw/write with without immediately cramping my hand.
#
# The Dolby Audio on the speakers leaves a lot to be desired. Some tweaking was needed to get the overblown sounds to subside. During the initial setup, it was hard to listen to. Turning Dolby Audio off though somehow reduces the speaker volume by 2/3... Not the actual volume percentage, but the actual sound. Dolby is using some sort of gain control so 20% volume sounds more like 40%.
#
# 4k video on YouTube looks great and only dropped 3 frames in a 2:44 video (Borderlands 3 FLAK FTW!) but that could have been from me trying to make it full-screen. I have not tried any games yet, but I did install Diablo 3, and World of Warcraft Classic. If anyone is interested, I will update this when I try them.
#
# With the Vega graphics, there is an automatic 2GB reduction in RAM availability. Couple this with the 8GB model and you are left with 6GB to run Windows 10 and any applications. Budget for the 12 GB model unless you are A) skilled with laptop hardware, and B) don't care about the warranty. I've seen the tear-downs, and there is an open RAM slot. I usually don't tear open electronics until a year after I own it (unless there is a reason to do so).
#
# The WiFi issue that is detailed on the 8GB model is non-existent for me. I feel like I may be more qualified to test that as Citrix requires a constant network connection and gives an immediate alert if connectivity drops for more than a couple seconds. No issues here, running the latest Windows updates.
#
# Overall I'm happy with this laptop, It is much lighter than my 15‚Äù Acer Predator and looks a lot more professional when traveling for work.
# """

review_str = """
the hp stream 14 has a lightweight plastic chassis measuring 13.3 x 8.9 x 0.70 inches and weighing just 3.11 pounds
the stream 14 keeps the price low by sticking to the bare essentials : a low - powered cpu , minimal ram and 32 gb of flash memory for storage"""
import networkx as nx
import re
import spacy
import itertools

stopwords = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very',
             'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most',
             'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until',
             'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this',
             'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when',
             'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then',
             'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself',
             'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',
             'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']


def read_article(filedata):
    article = filedata.split(".")
    sentences = []

    for sentence in article:
        print(sentence)
        replacedstr = re.sub('[^a-zA-Z]', ' ', sentence)
        sentences.append(replacedstr)
    sentences.pop() 
    
    return sentences

nlp = spacy.load('en_core_web_sm')
doc = nlp(review_str)

# for entity in doc.ents:
#   print(entity.label_, ' | ', entity.text)
#
# filter_label = ["CARDINAL", "DATE"]
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]

def keep_token(tok):
    return tok.pos_ not in {'PUNCT', 'NUM', 'SYM'}


def keep_entity(tok):
    tokens = []
    for e in doc.ents:
        if e[1] not in {'CARDINAL'}:
            tokens.append(tok)
    tokens.pop()
    return tokens
#
# pos_drop_tokens = list(filter(keep_token, doc))
# entity_keep_tokens = list(filter(keep_entity,pos_drop_tokens))
#

pos_drop_tokens = list()
entity_keep_tokens = list()
for p in read_article(review_str):
    docp = nlp(p)
    pos_drop_tokens.append(list(filter(keep_token, docp)))     
for p in pos_drop_tokens:
    list1 = [str(i) for i in p]
    s = " ".join(list1)
    docx = nlp(s)
    entity_keep_tokens.append(list(filter(keep_entity, docx)))

graphs = []
nountokens = []
adjtokens = []
# https://spacy.io/docs/usage/processing-text
for p in read_article(review_str):
    document = nlp(p)
    listofnountoken = []
    listofadjtoken =[]
    for token in document:
        if (token.pos_ in {'NOUN'}):
            # listofnountoken.append(token.text.lower() + "--" + str(token.i))
            listofnountoken.append(token.text.lower())
        if (token.pos_ in {'ADJ'}):
            # listofadjtoken.append(token.text.lower() + "--" + str(token.i))
            listofadjtoken.append(token.text.lower())
    print("Noun Token: ",listofnountoken)
    print("Adjective Token: ", listofadjtoken)
    nountokens.append(listofnountoken)
    adjtokens.append(listofadjtoken)
    print('document: {0}'.format(document))
    edges = []
    for token in document:
        # FYI https://spacy.io/docs/api/token
        for child in token.children:
            # edges.append(('{0}--{1}'.format(token.lower_,token.i),
            #           '{0}--{1}'.format(child.lower_,child.i)))
            edges.append(('{0}'.format(token.lower_),
                          '{0}'.format(child.lower_)))
    graphs.append(nx.Graph(edges))


# list(graphs[3].nodes(data=True))

shortestpaths = []
for i in range(len(graphs)):
    print("===============" + str(i) + "===============")
    print("Noun tokens: ")
    print(nountokens[i])
    print("Adjective tokens: ")
    print(adjtokens[i])
    if (len(adjtokens[i]) != 0 and len(nountokens[i]) != 0):
        try:
            for j in itertools.product(adjtokens[i], nountokens[i]):
                print(j)
                print(nx.shortest_path(graphs[i], source=str(j[0]), target=str(j[1])))
                shortestpaths.append(nx.shortest_path(graphs[i], source=str(j[0]), target=str(j[1])))
        except nx.NetworkXNoPath:
            print("Failed")
    else:
        print("Either adjective or noun token list is empty ")

shortestpathsset = set()

for i in range(len(shortestpaths)):
    filtered_sentence = []
    ret_str = ''
    for j in range(len(shortestpaths[i])):
        if shortestpaths[i][j] in stopwords:
            continue
        else:
            ret_str = ret_str + " " + shortestpaths[i][j]
    shortestpathsset.add(ret_str)

for item in shortestpathsset:
    print(item)